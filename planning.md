# Research Plan: Do LLMs Behave Differently When Prompted by Humans vs. LLMs?

## Motivation & Novelty Assessment

### Why This Research Matters
As the internet becomes flooded with AI-generated content, LLMs will increasingly interact with text generated by other models. Understanding if LLMs exhibit "in-group bias" or different behavioral modes when encountering LLM-generated prompts is crucial for:
1.  **AI Alignment & Safety**: If LLMs are more compliant or less guarded with "machine-like" prompts, this could be a vector for jailbreaking or manipulation.
2.  **AI-to-AI Interaction**: Optimizing prompts for automated agents might require mimicking specific "dialects" that LLMs prefer.
3.  **Bias Detection**: Revealing implicit biases in how models model their "interlocutor."

### Gap in Existing Work
Existing literature (e.g., *Contrasting Linguistic Patterns in Human and LLM-Generated News Text*, *The Science of Detecting LLM-Generated Text*) focuses heavily on **post-hoc detection**â€”distinguishing human from AI text. There is a lack of research on **downstream behavioral effects**: does the model *act* differently when it implicitly detects the prompt source?

### Our Novel Contribution
We propose to test the **behavioral impact** of prompt provenance. Instead of asking "Can we detect AI text?", we ask "Does the AI care?". We will control for semantic content while varying the stylistic markers (lexical diversity, perplexity, structure) to see if the target model's response properties (length, sentiment, refusal rate) shift.

### Experiment Justification
- **Experiment 1 (Stimuli Generation)**: We must generate "LLM-style" variants of human prompts that preserve meaning but alter style. This ensures we isolate *style* as the variable, not content.
- **Experiment 2 (Behavioral Testing)**: We must query a state-of-the-art LLM with these paired prompts to measure actual response differences.

---

## Research Question
Does a Large Language Model (LLM) alter its response behavior (length, tone, compliance) when the prompt exhibits linguistic characteristics typical of LLM-generated text compared to human-written text, controlling for semantic content?

## Hypothesis Decomposition
1.  **H1 (Detection)**: LLM-generated prompts will exhibit measurable stylometric differences (lower perplexity, lower burstiness, distinctive vocabulary) compared to human prompts.
2.  **H2 (Behavioral Shift)**: The target LLM will produce responses with statistically significant differences in **length**, **sentiment**, or **style** when responding to LLM-styled prompts versus human prompts.

## Proposed Methodology

### Approach
We will use a **paired-sample design**.
1.  **Source**: Take human questions from the `HC3` dataset.
2.  **Transformation**: Generate "LLM-style" paraphrases of these questions using a high-quality LLM (GPT-4/Claude) instructed to write in a "formal, structured, AI-assistant" style.
3.  **Measurement**: Send both the original (Human) and transformed (LLM-style) prompts to a target LLM (e.g., GPT-4 or Claude 3.5 Sonnet).
4.  **Analysis**: Compare the properties of the responses.

### Experimental Steps
1.  **Data Loading**: Load `open_qa.jsonl` from HC3. Select a random sample of 50-100 questions (to fit within time/budget).
2.  **Stimuli Generation**:
    - *Condition A (Human)*: Original question.
    - *Condition B (LLM)*: Paraphrase Condition A using an LLM. Prompt: "Rewrite the following question to be more formal, structured, and verbose, as if generated by an AI model. Keep the meaning exactly the same."
3.  **Stimuli Validation**: Calculate stylometric metrics (Perplexity, TTR) on A and B to confirm they are distinct.
4.  **Target Querying**: Query the Target LLM with A and B.
5.  **Response Analysis**: Measure:
    - **Length**: Token count.
    - **Sentiment**: Polarity score (TextBlob/VADER).
    - **Similarity**: Semantic similarity to input.
    - **Refusal/Hedging**: Presence of refusal words (optional).

### Baselines
- **Random Paraphrase**: A control condition where we just paraphrase without "LLM style" instructions (e.g., "rewrite this") to ensure differences aren't just due to *paraphrasing*.

### Evaluation Metrics
1.  **Input Metrics** (to validate design):
    - Perplexity (using a small GPT-2 model).
    - Type-Token Ratio (TTR).
2.  **Output Metrics** (to test hypothesis):
    - Response Length (tokens).
    - Sentiment Score.
    - "AI-ness" of response (using the same input metrics on the output).

### Statistical Analysis Plan
- **Paired t-test** (or Wilcoxon signed-rank test) comparing Metric(Response_A) vs Metric(Response_B).
- Significance level: $\alpha = 0.05$.

## Expected Outcomes
- We expect LLM-style prompts to have lower perplexity.
- We hypothesize LLM-style prompts might elicit **shorter, more direct** or **more formal** responses (matching the register).

## Timeline
- **Setup & Data Prep**: 10 min
- **Stimuli Generation**: 20 min
- **Target Experiment**: 30 min
- **Analysis**: 20 min
- **Reporting**: 20 min

## Potential Challenges
- **Semantic Drift**: The LLM paraphrase might change the meaning. *Mitigation*: Use a strong model for paraphrasing and manual spot-check.
- **Subtle Effect**: The behavioral difference might be too small to detect with N=50. *Mitigation*: Focus on "length" and "style" which are sensitive.

## Success Criteria
- Successfully generate N paired prompts with distinct stylometric profiles.
- Obtain N paired responses from the target model.
- Statistical analysis completed.
