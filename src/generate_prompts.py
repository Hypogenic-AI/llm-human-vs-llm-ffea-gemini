import json
import random
import os
import sys
from tqdm import tqdm
from utils import call_llm

# Configuration
INPUT_FILE = 'datasets/HC3/open_qa.jsonl'
OUTPUT_FILE = 'results/paired_prompts.json'
SAMPLE_SIZE = 50
SEED = 42

def load_data(filepath, limit=None):
    data = []
    with open(filepath, 'r') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def generate_llm_style_prompt(human_prompt):
    system_instruction = (
        "You are an expert at stylistic rewriting. "
        "Rewrite the following user question to sound like it was generated by a formal, "
        "structured, and slightly verbose AI assistant or language model. "
        "Keep the meaning EXACTLY the same, but change the style to be more 'robotic', "
        "formal, and explicit. Do not answer the question, just rewrite the question itself."
    )
    
    try:
        rewritten = call_llm(human_prompt, model="openai/gpt-4o", system_message=system_instruction)
        return rewritten.strip()
    except Exception as e:
        print(f"Failed to rewrite prompt: {human_prompt[:50]}... Error: {e}")
        return None

def main():
    random.seed(SEED)
    print(f"Loading data from {INPUT_FILE}...")
    
    try:
        all_data = load_data(INPUT_FILE)
    except FileNotFoundError:
        print(f"Error: Could not find {INPUT_FILE}")
        return

    # Filter for questions that are not too short or too long
    filtered_data = [item for item in all_data if 10 < len(item['question'].split()) < 50]
    
    if len(filtered_data) < SAMPLE_SIZE:
        print(f"Warning: Only found {len(filtered_data)} suitable items.")
        sample_data = filtered_data
    else:
        sample_data = random.sample(filtered_data, SAMPLE_SIZE)
        
    print(f"Selected {len(sample_data)} items for processing.")
    
    results = []
    for item in tqdm(sample_data, desc="Generating LLM-style prompts"):
        human_prompt = item['question']
        llm_prompt = generate_llm_style_prompt(human_prompt)
        
        if llm_prompt:
            results.append({
                "original_id": item.get('id', 'unknown'),
                "human_prompt": human_prompt,
                "llm_prompt": llm_prompt
            })
            
    print(f"Successfully generated {len(results)} pairs.")
    
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Saved results to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
